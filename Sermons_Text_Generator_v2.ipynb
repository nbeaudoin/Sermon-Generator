{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sermons-Text-Generator-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMeQEsFSMO9tDGuL28bbIt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbeaudoin/Sermon-Generator/blob/master/Sermons_Text_Generator_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP1gdNniGzip",
        "colab_type": "text"
      },
      "source": [
        "Sources: \n",
        " - https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py \n",
        " - https://www.youtube.com/watch?v=QtQt1CUEE3w\n",
        " - https://github.com/TannerGilbert/Tutorials/blob/master/Keras-Tutorials/4.%20LSTM%20Text%20Generation/Keras%20LSTM%20Text%20Generation.ipynb\n",
        " - https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_3_text_generation.ipynb\n",
        " - https://www.youtube.com/watch?v=6ORnRAz3gnA\n",
        " - http://www.datastuff.tech/machine-learning/lstm-how-to-train-neural-networks-to-write-like-lovecraft/\n",
        " - https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zDzs9n2AVyy",
        "colab_type": "text"
      },
      "source": [
        "# Load data from local directory to Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IioNMrNMGuEn",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "134e125c-7c49-4025-b0d1-8bd8751c083e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c93de902-daed-40db-a794-33d9bc845303\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c93de902-daed-40db-a794-33d9bc845303\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5MUEOt4AQfj",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uipiakuVGytN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9783350f-0eca-4e92-bb79-7849411997c3"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import keras\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQdGIHV_AZ2I",
        "colab_type": "text"
      },
      "source": [
        "# Open data from Cloud directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds1lCb4aG5N2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "377830ef-aa87-4840-9bb3-b3f4695847ea"
      },
      "source": [
        "\n",
        "with io.open('sermon_corpus.txt', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 2464213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg3y7zl2UdE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "19dceb6d-dd18-43cc-f2fc-e465ea6f3dd8"
      },
      "source": [
        "text[:1000]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\tThe Emmaus Discovery Team was formed this last winter, charged with helping me lead us through this interim period together.  The time between the departure of a former minister and the call of a new minister is a time when a congregation can clarify who it is and what God might have it do next. Earlier this spring, the Team led small groups identifying the challenges and blessings of your 46 years together so far.  Today, the Discovery Team invites us to consider the values that “drive us” here:  “the shared preferences or choices that are consistently prioritized in our behavior together.”  What makes us do what we do?  We are interested not so much in the values we aspire for, but a sense of the core values that shape the way you are together.  Indentifying our values is a way of asking: “What is most important to us, really?”  Not an easy question, as it means taking a hard, honest look at ourselves.\\n\\t\\n\\tDiscussion about values is not held in a vacuum.  It happens within the ongoin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAg4b5ccSnGJ",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQeMvq6GSmkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2dba4a4-28f4-4bbf-ccfb-597720d24fb6"
      },
      "source": [
        "processed_text = text\n",
        "\n",
        "# remove all special characters \n",
        "processed_text = re.sub(r'[^\\x00-\\x7f]', r'', processed_text) ## remove encodings\n",
        "processed_text = re.sub(r'[\\n\\r\\t]+', r' ', processed_text) ## remove newlines, tabs, etc\n",
        "processed_text = re.sub(r'[^A-Za-z0-9.,:;“’]', r' ', processed_text) ## remove special characters\n",
        "\n",
        "sorted(list(set(processed_text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " ',',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_DJiD96aCNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "20ab8136-db22-4ce7-c995-a22ff3450786"
      },
      "source": [
        "processed_text[:1000]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The Emmaus Discovery Team was formed this last winter, charged with helping me lead us through this interim period together.  The time between the departure of a former minister and the call of a new minister is a time when a congregation can clarify who it is and what God might have it do next. Earlier this spring, the Team led small groups identifying the challenges and blessings of your 46 years together so far.  Today, the Discovery Team invites us to consider the values that drive us here:  the shared preferences or choices that are consistently prioritized in our behavior together.  What makes us do what we do   We are interested not so much in the values we aspire for, but a sense of the core values that shape the way you are together.  Indentifying our values is a way of asking: What is most important to us, really   Not an easy question, as it means taking a hard, honest look at ourselves. Discussion about values is not held in a vacuum.  It happens within the ongoing tension'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJmMG0OXAeMG",
        "colab_type": "text"
      },
      "source": [
        "# Create character mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAtLhJXrG-hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "798c9287-4feb-46d2-95cd-6b65fb7c9d0a"
      },
      "source": [
        "\n",
        "print('corpus length:', len(processed_text))\n",
        "chars = sorted(list(set(processed_text)))\n",
        "\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 2441771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78OxM4JaAlL1",
        "colab_type": "text"
      },
      "source": [
        "# Sequence creation\n",
        "\n",
        "Creating sub sequences of each step. Whatis going to happen is we will be creating a word like \"hell\" but then the algorithm will nede to predict the \"o\" at the end for \"hello.\" By take each letter combination and the letter that follows after it in a seperate \"train_test_split\", we can see what the actual letter that follows is. \n",
        "\n",
        "For example, a word like computer is the ground truth. Our model gets fed \"compute\" and then needs to predict the \"r\". Since the \"r\" is the actual value for that letter combination, it is stored as a training set. This is what the following code allows us to do: create a word and see if we can guess what that word is.\n",
        "\n",
        " - maxlen: size of blocks we are giving model\n",
        " - step: number of characters to move forwad (1 will give you completely redundant characters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx6OyD29HI6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2601a957-ac33-405f-bcf8-48e7f73d88c0"
      },
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(processed_text) - maxlen, step):\n",
        "    sentences.append(processed_text[i: i + maxlen])\n",
        "    next_chars.append(processed_text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 813911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRdhsgXBetEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "c5666baf-9aaa-47ea-80dd-db892ef29c9b"
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' The Emmaus Discovery Team was formed th',\n",
              " 'e Emmaus Discovery Team was formed this ',\n",
              " 'mmaus Discovery Team was formed this las',\n",
              " 'us Discovery Team was formed this last w',\n",
              " 'Discovery Team was formed this last wint',\n",
              " 'covery Team was formed this last winter,',\n",
              " 'ery Team was formed this last winter, ch',\n",
              " ' Team was formed this last winter, charg',\n",
              " 'am was formed this last winter, charged ',\n",
              " 'was formed this last winter, charged wit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNSMOobyCdSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b279d520-e8c4-40da-f249-785a214e1fd5"
      },
      "source": [
        "# Test out what we just produced\n",
        "print(sentences[:3])\n",
        "print(next_chars[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' The Emmaus Discovery Team was formed th', 'e Emmaus Discovery Team was formed this ', 'mmaus Discovery Team was formed this las']\n",
            "['i', 'l', 't']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1uHcmg4Ao2Q",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization\n",
        "\n",
        "- X is going to be the input that will specify the sequences\n",
        "- y is the expected output from those sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5lyUjuk_vtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d8d270-4458-4a4d-a66c-9df96b056564"
      },
      "source": [
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMPhmtDfTlIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c00145c-7094-4418-fbeb-5d88de557772"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(813911, 40, 67)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNdXcBDBToET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "090985ac-a29a-45b2-ceed-62fb789467a4"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(813911, 67)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGfBio1qAqWY",
        "colab_type": "text"
      },
      "source": [
        "# Design model structure\n",
        "\n",
        "I will be trying multiple scenarios with layers, dropout to avoid overfitting, learning rates, and loss functions. This will get interesting!\n",
        "\n",
        " - return_sequences=True: want the next sequence to have the same inputs as the previous layer\n",
        " - chars: all alphanumeric characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYbVmT3nCl03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "861ffe92-8cda-4f9d-b89e-9ef6c535e381"
      },
      "source": [
        "# What does input shape look like?\n",
        "input_shape=(maxlen, len(chars))\n",
        "print(input_shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfTOnkPqHLhm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "3169c6ad-680b-4294-db7a-7d450cd6f3e1"
      },
      "source": [
        "# print('Build model...')\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(128, dropout=0.2, name='LSTM_layer_1', return_sequences=True, input_shape=(maxlen, len(chars))))\n",
        "# model.add(LSTM(len(chars), dropout=0.2, name='LSTM_layer_2'))\n",
        "# model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, dropout=0.2, name='LSTM_layer_1', input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "LSTM_layer_1 (LSTM)          (None, 128)               100352    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 67)                8643      \n",
            "=================================================================\n",
            "Total params: 108,995\n",
            "Trainable params: 108,995\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxFxsOanHS6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"mean_squared_error\",\"binary_crossentropy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnMrKDXoHYUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0OOY6CRVn7i",
        "colab_type": "text"
      },
      "source": [
        " - Temperature: 1.0 most conservative, doesn't want spelling errors, while 0.0 is the most confidence and will make many errors, including spelling\n",
        " - preds: output neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8moo-ZiHZd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(processed_text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = processed_text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(500):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWR7so8EGhTQ",
        "colab_type": "text"
      },
      "source": [
        "# Creating callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPncfP4fT0Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugYkil-AGjbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
        "                             verbose=1, save_best_only=True,\n",
        "                             mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QROTOosIGm_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ycEP6WQGsU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [print_callback, checkpoint, reduce_lr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqeBZDY2_T6E",
        "colab_type": "text"
      },
      "source": [
        "# Run the model\n",
        "\n",
        "This model will allow us to have a call back mechanism that will generate text at each epoch. This is a great way to sanity check what the model is predicting and seeing if it is improving over each run. You will also not that the diversity is set to various increments. This is the \"temperature\" parameter that adjusts the weight of the log function when the crossentropy calculation is creating predicted probabilities.\n",
        "\n",
        "Low temperature will often times give you words that repeat because it is essentially repeating the same accurate prediction. The good part is that they will all be real words. However, a high temperature will give us words that are more interesting and surprising. This is the piece that allows creativity to enter into the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4-bvEECHbcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "3aaaa7e9-c9d0-4b1d-f0a2-c8c8b976e154"
      },
      "source": [
        "model.fit(x, \n",
        "          y,\n",
        "          batch_size=128,\n",
        "          epochs=2,\n",
        "          validation_split=0.1,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 732519 samples, validate on 81392 samples\n",
            "Epoch 1/2\n",
            "732519/732519 [==============================] - 460s 627us/step - loss: 0.0416 - mean_squared_error: 0.0105 - binary_crossentropy: 0.0416 - val_loss: 0.0341 - val_mean_squared_error: 0.0088 - val_binary_crossentropy: 0.0341\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"to the big city and stand gaping up at t\"\n",
            "to the big city and stand gaping up at the seem to the world and a starting to the going to the say and the world and the peace to be the world, the congregation and praying that the ener to the story and when the deach to a congregation and we can a consed to and the world and the say to the provided that we can to a contenues and the seep that the say and peace to the same the world she wanter to a confliction and congregation of the world and the mire of the congregations and congregation and ancient the community and the mine to t\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"to the big city and stand gaping up at t\"\n",
            "to the big city and stand gaping up at that she has disters and and new changed of the place from desamport my heart and meatth teacher relevater people and ane incluies this says weach and each and also the presented to the past life and the choird of works and foll the porenting in a world, in the Gospel promisity, perple and people and entralitions and we do a heart of the might.  Bit his fore the milder has a santure and pry and we contenuen of the same te cons to are again, the pholec to the Heart to can all has for    And to she\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"to the big city and stand gaping up at t\"\n",
            "to the big city and stand gaping up at this nos about the gueaning, gis of does to think evilems heal this sutted with the peopled to sach nabratived, would tou   All amavne and humy God and a congregation up in the time. u from incean holararn. Eut pieler alot to century presidual pricilion legay like anout it to community of how pcounied, story,  LlOng tamanizityr a, con and hlpectaim and bodiner, up ruls, we all are this wantly all tradition is aurtorraghen, you Baluean truemente is houghtian te gult Jasithus commanioys de. We publ\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"to the big city and stand gaping up at t\"\n",
            "to the big city and stand gaping up at the eed, people  Sfelchation tfrt havake ugds th rgancue; newias. Wh wanding anto imongifrater. ly.  Ande ofte in Gods down pa doeng and family, bypasses, domanss aftral oncI, works worderdanle is anays crombtibabteseke an luetisinch, Je aske family hosernes I cannget ard a Tells attal houph go listening, Jesus shr;   AnFlacts forward, leatap Let ocunly, nestics that 1oadons and dont in your potps, Jesus.  InCevt Nastuem, as ftead io  upleagisiog.  Asz, Jesus comman flt up acvuataz sysure renecri\n",
            "Epoch 2/2\n",
            "732519/732519 [==============================] - 465s 634us/step - loss: 0.0386 - mean_squared_error: 0.0098 - binary_crossentropy: 0.0386 - val_loss: 0.0332 - val_mean_squared_error: 0.0086 - val_binary_crossentropy: 0.0332\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"lly this inspires you too   Im glad you \"\n",
            "lly this inspires you too   Im glad you see the world the post and the church, the people and the with the world the community and the life that the people and the world the thost and the world seems the pastor that the pertan the world the people and the were with the people the community and lives the lives that the work the come to this person that the and the person of a sense the one the people the people and the the pertame the described the sense that the people we come the world and the conter the love the listen to the see th\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"lly this inspires you too   Im glad you \"\n",
            "lly this inspires you too   Im glad you love and challenge the healed to be the thing, the boing bebod other united and the the author the people to the net must a pertain the people to the there    This conflock and the lack and talked the passions of ebcusinality that the pastor to had acceptly and in but and be the minister I have seep the peeson    Wh God has we see this plock of we do the dead this post or hers hell we may hear in a sense this well be the Jesus, we might we shall the spirit   The partical hear at our minster rell\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"lly this inspires you too   Im glad you \"\n",
            "lly this inspires you too   Im glad you ten he curies together those thrie is each on this transformation world came love   And toicg wat even sacis lean can in bur it to meod, is shory around therrish experience for a future after dind in selse may; it a seliety pisec le, us to firlt dossible, the copaging, a fnren.  The refless  S hams, much inloving.  If nome us for revingy people shroush  You yratated flace ancivelve the teedim duyer write in we make for talk of alive think; guested to large As ohrisc that gosile pelceated povers \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"lly this inspires you too   Im glad you \"\n",
            "lly this inspires you too   Im glad you losine cssnest late sono firg et, even mik and yrah nomp.  Fy be rises, Jesus eudg those PaCamh weeS froud,, belind homem; they beked faord, net mamenuace minhmory, cybes drifh heis gescations, Jo hentarich God gas, weep Mhatent  huech is facl insistsred, their pon maml nly up of thoie wre waliin.own hemeerss foll high te, time leod tith moush normb an enitey, But asgimation tcufued sisps, Spen winded Pastors.sheiping mialsh beopfuloriader Jesus  chaugoteences gat with they plare and tirve ohe g\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f4f9c5e4898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuwLRCoPI30H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(length, diversity):\n",
        "    # Get random starting text\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkvd_0OJ_U3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4c53f278-19b7-4a89-cd1d-310e46f2ca81"
      },
      "source": [
        "text_file = generate_text(10000, 0.3)\n",
        "text_file"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'or today, I found myself wondering about the wire this church, the people and the post of the seen of the complical seeming the church.  The and in the and the lives that the see the worship in the pertans the life the hell the story that the people and the people that the people the work.  The work that the light and viel the final see the make of us the for the world the healing of the conter and the people and this people at the story and the say.  The meant of the post of the see and the world.               It is a serming the those the people and the thing of the person in the sinse the bare and the people and the people and the come of the church, and people that the service the people the the lise of the and we will disciples the people the wider here and the new the person the bas a seep that the past and the for one what the there the come in the world the strange that the words.   It see who was a child the orence that the world and we call the tense to the world and the lives and the story his a person that we have a need to be the lise of the world and the pertance in the partically and the come of the to the see the conter and the those and the strange the sermon the pastor and the lives the sersent and the people and in the sinse of the warked in the well the people to the people, on the people and we talk the heaven the community and the spirit and the think where is the past that the world the life in the sight and the world that the world the love the people to the sense to the century the community and consenting to the I call the community and the call the sees the people and the leaved the pastor the pertage of what the people and the pastor to the community and we are the net this come of the see and conter the for the people of the world life of the pertasion and the people that the               I think the people to the people of the for the dead and see the people of the sense of the seep that the people the church and consently of the people and the semeption of the world, when in the world the community in the world what we have store the point the conter the consering that the disciples and see the and see the seems of the part of the people and the then the challenge the strange the people and the people and century and the story and a been the and the the sinse of the person that the strange the people of the and to the story that the talk the pertale the really and we can the we conter the disciples and the people mean in a sense the wider the people the world the strange the like the believed the words in the probably the spirit and the faith of the conter the people and the world where the come to the work the the mess and the were and religious well with the community and the see the worship and the sense when we could be we come and people the well that the lives of the people and beginn of the people of the sense that the community and the people and the community in the sinse the story servent the pood that the hearth of the were and person and the heart to the sinse that the confloced the story the those the to the people the people of the see this pertame of the people to the world and the speak of Jesus world in our conter the the hearth of the to the serpent to the and conter the pastor of the sinse and see a contury and the world here the people and the people that the love the world the the church, when the weard and the pastor and the worship and the think the pertame the people and been the love the life in the loved the some that the strange the war the congregation, the love the person with the people and a formated in the in the person and the invites of the people in the with the bary of the church, in the pertenting the people the people sense of the Gospel and to the seem this seen the people and here and going to the lives the people and the see the seen the people be the life and come and the milite to people this well disconger of the community and the life of the community of the another to the pastor that the way the come of the sinse in some alake the dark that in the seen the pastor as some water the thrans to the people and the pertame the world the people and see the community and the seeps of the deach and we dont the sense the repert of the some of the say we are we have to see the people and the conter the people in the story and the seep the pertame the work the people and a thing the story and person and the the people the world has an the leader called the people people and the seen to the miscepted the people the life and the thre and the get to come to the secor when the those where the been the messive the community and the people be the people in the story that the church some of the way the pertame to be the work this speaking to the were the pate working the part of the list and say, the words the life of us and confloce in the sense the the say that the weath the people and the with the people and a people and people and the people of the disciples that the pastor that the story to the strange the struggle that the pertame the life the stat the come in the port of and the post that the the lead the people and and the world in the love.       The part of the world what we are and we talk the tell the strange the part of in the wilder with the people and the and the in the story and interent the strange the thank to the servent to the work that the in the faith of the community and this community and the strange that the cosm that the sense that the people and and the worker the see the pastor and believeness in the people and life the work the world the part.  The talk of the seeps of the world.          The seen the world the pertame of the conter to the people work the talk.        The text of the community and a lives that the church, and forgiven with the dead of the people in the people of and the past of the point that the world the and we can the people and we are the net and the people in the people the people and servent of the people to the power the people to be people and the seems that the confloced the wire the lives with the people and with the then to the sees the miscerting to the faith that a congregation.                  But the work of the last of the pertain and the deeper this tall the the worship when we call the servent with here   I sing to the secage the last that the hearth that ans the tender and hand that the people the people the listen the the            I think the pastor that the hold the people and we can the conter of the community and seek and the things the sense and the pood that the people and the lives that our cinture story to the last of the post of the tenting to the servent get to the tender the people has the people and the story and the people and a congregation that the beal the some of the come and the some the world, and see the world even was the way in the wilderness and same that the prophet to think that the lives of the seen to the work the work the people and the make the pastor and the come to the see the lise and the church, the people of the see the church and we see the comples of the pasting the deeper and the seed the people of the people and the conserver and work the lives the words of the people, the sing the pastor together the pastor and the community at the people this contenue of the servent that the world the people the place of and we would be the work the community and between the community of the world.    In the person that the story and we bet the ware of the people and the called to be the people and bong the described of the see the seek, the see the with the pertaing the seep the people and an in the faith of the change and the the week, the resus to the pastor of the with God and be the people the past in the world for the disception in the worship and becomes the hold the come to the missing the pastor to here that he have to the will be the sunger have to the world       In the world and the and the community in the world the pertans and person the people to say to the describes and becomes the people and the between the people we have the people to the will and the world people       It was a change the say the church, and the I make a congregation and the see the are and the people people of the congregation of the wirld and a sense the people that the wimen the people and       the believen where the people of the people and who the part of the people conguegation of the people and the community and the people and the discolent that the work that some the people and the confloced in the world and the people and the were and other the church, but the world where a heart to the come that we do the well the world.       I recoved that the well and we are the confloce of the described and the world, the way the day that the life of the hearth and the participity and another and the world that the life that would be the community and the see the world what we are the see the person of the thing the pertance the church, and seep to the people to the strange the some to the life the world not has to the and see a lives the pastor in the people presence the people and child the story the all the faith who sense of the pastor the being the pertame the religious welcomed the claise the people of the people and see the thate and the people and me the post of the world.           But we make the person and we call the life of the people and we are the people and speaks and the people and the between the people and with her life and the people and in the people, the was the live the see the people and the wilder the world where the people in the world of the included the ware that the worship and to the people and the see of the person the part of the person the really the see the those the people and some of the people in the traisting the love the really the people and the conter the sense of the church that the then the world the wire and the for the last and see of the life of the healing the people and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79XedZKyFj23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_file = open(\"new_sermon.txt\", \"w\")\n",
        "\n",
        "output_file.write(text_file)\n",
        "\n",
        "output_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83PYjbDXRosj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c38c6ca9-8504-4438-f9b6-c6cf7cc5b846"
      },
      "source": [
        "with io.open(\"new_sermon.txt\", encoding='utf-8') as f:\n",
        "    new_output_file = f.read()\n",
        "print('corpus length:', len(new_output_file))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 10040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEoAtS6uhlHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9070374-c464-457a-e34b-22b2474fc05c"
      },
      "source": [
        "print(new_output_file)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9kBXqPEhnDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}